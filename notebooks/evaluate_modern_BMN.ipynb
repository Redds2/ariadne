{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_169098/51594785.py:11: DeprecationWarning: Importing clear_output from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output, display\n",
      "/tmp/ipykernel_169098/51594785.py:11: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import clear_output, display\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from IPython.core.display import clear_output, display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "    \n",
    "from eval.event_evaluation import EventEvaluator\n",
    "from ariadne_v2.transformations import Compose, DropShort, DropTracksWithHoles, DropSpinningTracks, BakeStationValues, ConstraintsNormalize, FixStationsBMN\n",
    "from ariadne.utils.model import get_checkpoint_path, weights_update\n",
    "\n",
    "parse_cfg = {\n",
    "    'csv_params' : {\n",
    "        \"sep\": '\\s+',\n",
    "        \"encoding\": 'utf-8',\n",
    "        \"names\": ['event',  'x', 'y', 'z', 'det','station', 'track', 'px', 'py', 'pz', 'vx', 'vy', 'vz']\n",
    "    },\n",
    "    'input_file_mask':'/zfs/store5.hydra.local/user/p/pgonchar/data/bmn/run7/events/simdata_ArPb_3.2AGeV_mb_46.txt',\n",
    "    'events_quantity':'1..2000'\n",
    "}\n",
    "\n",
    "z_values = {0:12.344, 1: 15.614, 2: 24.499, 3: 39.702, 4: 64.535, 5: 112.649, 6: 135.330,7: 160.6635, 8: 183.668}\n",
    "constraints = {'x': [-81.1348, 86.1815], 'y': [-27.17125, 38.90473], 'z': [11.97, 183.82]}\n",
    "\n",
    "global_transformer = Compose([\n",
    "    FixStationsBMN(),\n",
    "    DropShort(num_stations=4),\n",
    "    DropTracksWithHoles(),\n",
    "    DropSpinningTracks(),\n",
    "    BakeStationValues(values=z_values),\n",
    "    ConstraintsNormalize(constraints=constraints),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.clean_cache\n",
    "\n",
    "#to clean cache if needed\n",
    "#scripts.clean_cache.clean_jit_cache('20d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import faiss\n",
    "\n",
    "NUM_POINTS_TO_SEARCH = 5\n",
    "\n",
    "from ariadne_v2.inference import IModelLoader\n",
    "from ariadne.tracknet_v2.model import TrackNETv2\n",
    "class TrackNetModelLoader(IModelLoader):    \n",
    "    def __call__(self):\n",
    "        tracknet_input_features=3\n",
    "        tracknet_conv_features=32\n",
    "        \n",
    "        tracknet_ckpt_path_dict = {'model_dir': '/zfs/hybrilit.jinr.ru/user/d/drusov/ariadne/lightning_logs/TrackNETv2', \n",
    "                                   'version': 'OnlyRNN_alpha0.01_unbalanced', 'checkpoint': 'latest'}\n",
    "        path_to_tracknet_ckpt = get_checkpoint_path(**tracknet_ckpt_path_dict)\n",
    "\n",
    "        model = weights_update(model=TrackNETv2(input_features=tracknet_input_features,\n",
    "                                        conv_features=tracknet_conv_features,\n",
    "                                        rnn_type='gru',\n",
    "                                        batch_first=True,\n",
    "                                        use_causalconv=False,\n",
    "                                        use_rnn=True),\n",
    "                       checkpoint=torch.load(path_to_tracknet_ckpt, map_location=torch.device(DEVICE)))\n",
    "        model.eval()\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        model_hash = {\n",
    "            \"tracknet_ckpt_path_dict\":path_to_tracknet_ckpt,\n",
    "            'gin':gin.config_str(), \n",
    "            'model': '%r' % model,\n",
    "            'NUM_POINTS_TO_SEARCH':NUM_POINTS_TO_SEARCH\n",
    "        }\n",
    "        return model_hash, [model]\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "import ariadne.transformations  as trn\n",
    "\n",
    "tracknet_transformer = trn.Compose([\n",
    "    #trn.DropShort(num_stations=4),\n",
    "    #trn.DropTracksWithHoles(),\n",
    "    #trn.DropSpinningTracks(),\n",
    "    #trn.BakeStationValues(values=z_values),\n",
    "    #trn.ConstraintsNormalize(constraints=constraints),\n",
    "    ]\n",
    ")\n",
    "\n",
    "NUM_COMPONENTS = 2\n",
    "SUFX = ['_p', '_c']\n",
    "COLS = ['x', 'y', 'z']\n",
    "\n",
    "def build_index(target_df):\n",
    "    cont = np.ascontiguousarray(target_df[COLS].values)\n",
    "    return store_in_index(cont)\n",
    "\n",
    "def build_hits(target_df):\n",
    "    cont = np.ascontiguousarray(target_df[COLS].values)\n",
    "    return cont\n",
    "\n",
    "def preprocess_one_event(event_df, n_neighbours=5):\n",
    "    try:\n",
    "        event_df = tracknet_transformer(event_df)\n",
    "    except AssertionError as err:\n",
    "        #print(\"ASS error %r\" % err)\n",
    "        return None\n",
    "    return event_df\n",
    "\n",
    "times = []\n",
    "\n",
    "def run_tracknet_eval(result_from_prev_step, model, n_neighbours=5):\n",
    "    timestart = time.time()\n",
    "    model = model[0]\n",
    "    preds, labels, ellipses = go_over_stations(result_from_prev_step, model, n_neighbours)\n",
    "    times.append(time.time() - timestart)\n",
    "    result = build_df(preds, labels, result_from_prev_step)\n",
    "    for col in [f'hit_id_{i}' for i in range(9)]:\n",
    "        result[col] = result[col].astype(int)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ariadne.utils.base import store_in_index,search_in_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def go_over_stations(df, model, n_neighbours, max_n_stations=9):\n",
    "    max_batch_size = 256\n",
    "    all_hits_index = build_index(df)\n",
    "    tracks_vs_len, all_tracks, multiplicity = get_tracks(df)\n",
    "    #print(tracks_vs_len)\n",
    "    #seeds, target = to_cart(df)\n",
    "    chunk_data_x = get_seeds_only_first_station(df)\n",
    "    #print(chunk_data_x)\n",
    "    #chunk_data_x = np.expand_dims(chunk_data_x, 1)\n",
    "    #index = build_index(target)\n",
    "    # search(target[:2], index)\n",
    "    #chunk_data_x = seeds_to_input(seeds)\n",
    "    chunk_data_len = torch.tensor(np.full(len(chunk_data_x), 1), dtype=torch.int64).to(DEVICE)\n",
    "    gru_candidates = []\n",
    "    x_candidates = []\n",
    "    candidate_labels = []\n",
    "    candidate_ellipses = []\n",
    "    for stations_gone in range(1, max_n_stations):\n",
    "        #print(f'===> {stations_gone}')\n",
    "        num_batches = int(len(chunk_data_x) / max_batch_size) + 1\n",
    "        #print(num_batches)\n",
    "        num_right_batches = 0\n",
    "        num_all_batches = 0\n",
    "        next_stage_xs = []\n",
    "        next_stage_lens = []\n",
    "        if len(chunk_data_x) == 0:\n",
    "            #print('Have zero ellipces on this station! Skipping other stations')\n",
    "            #print(candidate_ellipses)\n",
    "            #print(x_candidates)\n",
    "            #print(candidate_labels)\n",
    "            return x_candidates, candidate_labels, candidate_ellipses\n",
    "        labels_for_empty_el_batch = []\n",
    "        labels_for_full_el_batch = []\n",
    "        station_df = df[df['station']==stations_gone]\n",
    "        current_index = build_index(station_df)\n",
    "        this_station_hits = build_hits(station_df)\n",
    "        for batch_num in range(num_batches):\n",
    "            min_i = max_batch_size*batch_num\n",
    "            max_i = min(max_batch_size + min_i, len(chunk_data_x))\n",
    "            if min_i==max_i:\n",
    "                #print('Have zero ellipces on this station! Skipping other stations')\n",
    "                #print(candidate_ellipses)\n",
    "                #print(x_candidates)\n",
    "                #print(candidate_labels)\n",
    "                return x_candidates, candidate_labels, candidate_ellipses\n",
    "\n",
    "            this_batch_x = torch.tensor(chunk_data_x[min_i: max_i]).to(DEVICE)\n",
    "            this_batch_len = chunk_data_len[min_i: max_i]\n",
    "            #print(this_batch_x)\n",
    "            batch_prediction, batch_gru = model(this_batch_x.float(),\n",
    "                                                     torch.tensor(this_batch_len, dtype=torch.int64).to(DEVICE),\n",
    "                                                     return_gru_states=True)\n",
    "            new_pred = torch.full((len(batch_prediction), 5), z_values[stations_gone], device=DEVICE)\n",
    "            new_pred[:, :2] = batch_prediction[:, -1, :2]\n",
    "            new_pred[:, 3:] = batch_prediction[:, -1, 2:]\n",
    "            batch_prediction = new_pred\n",
    "            batch_gru = batch_gru[:, -1]\n",
    "            #if batch_num == 0:\n",
    "              #  print(f'station {stations_gone+1}, on this station: {len(this_station_hits)} hits' )\n",
    "            if len(this_station_hits) == 0:\n",
    "            #    LOGGER.info('Have zero hits on this station! Skipping other stations')\n",
    "            #    print(candidate_ellipses)\n",
    "            #    print(x_candidates)\n",
    "            #    print(candidate_labels)\n",
    "                return x_candidates, candidate_labels, candidate_ellipses\n",
    "            prediction_numpy = batch_prediction.detach().cpu().numpy()\n",
    "            nearest_hits_index = search_in_index(prediction_numpy[:, :3],\n",
    "                                                 current_index, \n",
    "                                                 n_neighbours,\n",
    "                                                 n_dim=3)\n",
    "            nearest_hits = this_station_hits[nearest_hits_index]\n",
    "            nearest_hits, in_ellipse_mask = filter_hits_in_ellipses(prediction_numpy,\n",
    "                                                               nearest_hits,\n",
    "                                                               nearest_hits_index,\n",
    "                                                               filter_station=False,\n",
    "                                                               z_last=True,\n",
    "                                                               find_n=nearest_hits_index.shape[1])\n",
    "            nearest_hits = torch.from_numpy(nearest_hits)\n",
    "            nearest_hits_mask = in_ellipse_mask \n",
    "            # here empty ellipses and all inputs for them are saved\n",
    "            empty_xs, empty_ellipses, empty_grus, predicted_ellipses = get_data_for_empty_ellipses(this_batch_x, \n",
    "                                                                               batch_prediction, \n",
    "                                                                               batch_gru, \n",
    "                                                                               nearest_hits_mask)\n",
    "            prolonged_batch_xs, prolonged_grus = prolong(this_batch_x,\n",
    "                                                         batch_gru,\n",
    "                                                         nearest_hits_mask,\n",
    "                                                         nearest_hits,\n",
    "                                                         stations_gone,\n",
    "                                                         use_torch=True)\n",
    "            next_stage_xs.append(prolonged_batch_xs)\n",
    "            next_stage_lens.append(np.full(len(prolonged_batch_xs), stations_gone + 1))\n",
    "            if stations_gone > 3:\n",
    "                use_empty_ellipses = []\n",
    "                empty_ellipses_station_intersections = []\n",
    "                use_empty_ellipses = torch.ones(len(empty_ellipses), dtype=torch.bool)\n",
    "                empty_xs = empty_xs[use_empty_ellipses]\n",
    "                empty_grus = empty_grus[use_empty_ellipses]\n",
    "                if len(empty_xs) > 0:\n",
    "                    (candidate_labels,\n",
    "                     labels_for_empty_el_batch,\n",
    "                     gru_candidates,\n",
    "                     x_candidates,\n",
    "                     candidate_ellipses) =  get_candidates(empty_xs, \n",
    "                                                     tracks_vs_len[stations_gone],\n",
    "                                                     prolonged_grus[:, -2], \n",
    "                                                     empty_ellipses,\n",
    "                                                     all_hits_index,\n",
    "                                                     candidate_labels, \n",
    "                                                     labels_for_full_el_batch, \n",
    "                                                     gru_candidates, \n",
    "                                                     x_candidates,\n",
    "                                                     candidate_ellipses)\n",
    "                    #print(candidate_ellipses)\n",
    "            if (stations_gone == (max_n_stations-1)): # if we are predicting for last station\n",
    "                if len(prolonged_batch_xs) > 0:  # now we prolong candidates with all hits *in* ellipses\n",
    "                    (candidate_labels,\n",
    "                     labels_for_full_el_batch, \n",
    "                     gru_candidates, \n",
    "                     x_candidates,\n",
    "                     candidate_ellipses) =  get_candidates(prolonged_batch_xs, \n",
    "                                                     tracks_vs_len[stations_gone + 1],\n",
    "                                                     prolonged_grus[:, -1], \n",
    "                                                     predicted_ellipses,\n",
    "                                                     all_hits_index,\n",
    "                                                     candidate_labels, \n",
    "                                                     labels_for_full_el_batch, \n",
    "                                                     gru_candidates, \n",
    "                                                     x_candidates,\n",
    "                                                     candidate_ellipses)\n",
    "\n",
    "        if len(next_stage_xs) > 1:\n",
    "            chunk_data_x = np.concatenate(next_stage_xs, 0)\n",
    "            chunk_data_len = np.concatenate(next_stage_lens, 0)\n",
    "        else:\n",
    "            try:\n",
    "                chunk_data_x = next_stage_xs[0]\n",
    "                chunk_data_len = next_stage_lens[0]\n",
    "            except:\n",
    "                continue\n",
    "    return x_candidates, candidate_labels, candidate_ellipses\n",
    "\n",
    "def get_tracks(df, min_len=4):\n",
    "    tracks = df[df['track'] != -1].groupby('track')\n",
    "    multiplicity = tracks.ngroups\n",
    "    tracks_vs_len = {3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []}\n",
    "    all_tracks = []\n",
    "    for i, track in tracks:\n",
    "        temp_track = track[['x', 'y', 'z']].values\n",
    "        if len(temp_track) >= min_len:\n",
    "            tracks_vs_len[len(temp_track)].append(temp_track)\n",
    "            all_tracks.append(temp_track)\n",
    "    for stations_in_track, this_track_list in tracks_vs_len.items():\n",
    "        if len(this_track_list) > 0:\n",
    "            tracks_vs_len[stations_in_track] = np.stack(this_track_list, 0)\n",
    "    return tracks_vs_len, all_tracks, multiplicity\n",
    "\n",
    "def get_seeds_only_first_station(df, columns=['x','y','z']):\n",
    "    real = df#[df['track']!=-1]\n",
    "    temp1 = real[real.station == 0]\n",
    "    st0_hits = temp1[columns].values\n",
    "    # all possible combinations\n",
    "    seeds = np.zeros((len(temp1), 1, 3))\n",
    "    seeds[:, 0, ] = st0_hits\n",
    "    return seeds\n",
    "\n",
    "def filter_hits_in_ellipses(ellipses, nearest_hits, hits_index, z_last=True, filter_station=True, find_n=10):\n",
    "    \"\"\"Function to get hits, which are in given ellipse.\n",
    "    Space is 3-dimentional, so either first component of ellipce must be z-coordinate or third.\n",
    "    Ellipse semiaxises must include x- and y-axis.\n",
    "    Arguments:\n",
    "        ellipse (np.array of size 5): predicted index with z-component like\n",
    "                                      (x,y,z, x-semiaxis, y_semiaxis) or (z, x,y, x-semiaxis, y_semiaxis)\n",
    "        nearest_hits (np.array of shape (n_hits, 3) or only 3): some hits in 3-dim space\n",
    "        z_last (bool): If True, first component of vector is interpreted as z, if other, third.\n",
    "        filter_station (bool): if True, only hits with same z-coordinate are considered, else all hits\n",
    "    Returns:\n",
    "        numpy.ndarry with filtered hits, all of them in given ellipse, sorted by increasing of distance\n",
    "    \"\"\"\n",
    "    assert nearest_hits.shape[-1] == 3, \"index is 3-dimentional, please add z-coordinate to centers\"\n",
    "    if nearest_hits.ndim < 2:\n",
    "        nearest_hits = np.expand_dims(nearest_hits, 0)\n",
    "    if nearest_hits.ndim < 3:\n",
    "        nearest_hits = np.expand_dims(nearest_hits, 0)\n",
    "    assert ellipses.shape[-1] == 5, \"index is 3-dimentional, you need to provide z-coordinate (z_c, x_c, y_c, x_r, y_r) or (x_c, y_c, z_c, x_r, y_r)\"\n",
    "    #ellipses = np.expand_dims(ellipses, -1)\n",
    "    #find_n = len(nearest_hits)\n",
    "    ellipses = np.expand_dims(ellipses, 2)\n",
    "    #found_hits = nearest_hits.reshape(-1, find_n, nearest_hits.shape[-1])\n",
    "    if z_last:\n",
    "        x_part = (ellipses[:,0].repeat(find_n,1) - nearest_hits[:, :, 0]) / ellipses[:, 3].repeat(find_n,1)\n",
    "        #print(x_part**2)\n",
    "        y_part = (ellipses[:,1].repeat(find_n,1) - nearest_hits[:, :, 1]) / ellipses[:, 4].repeat(find_n,1)\n",
    "        #print(y_part**2)\n",
    "    else:\n",
    "        x_part = (nearest_hits[:, :, 1] - ellipses[:, 1].repeat(find_n, 1)) / ellipses[:, -2].repeat(find_n, 1)\n",
    "        y_part = (nearest_hits[:, :, 2] - ellipses[:, 2].repeat(find_n, 1)) / ellipses[:, -1].repeat(find_n, 1)\n",
    "    left_side = x_part**2 + y_part**2\n",
    "    is_in_ellipse = left_side <= 1\n",
    "    is_in_ellipse *= hits_index != -1\n",
    "    return nearest_hits, is_in_ellipse\n",
    "\n",
    "def get_data_for_empty_ellipses(x, preds, grus, mask):\n",
    "    empty_ellipses_mask = (mask.sum(axis=-1) == 0) # if nothing is found in ellipse\n",
    "    empty_ellipses = preds[empty_ellipses_mask]#.detach().cpu().numpy()\n",
    "    full_ellipses = preds[~empty_ellipses_mask]\n",
    "    empty_xs = x[empty_ellipses_mask]\n",
    "    empty_grus = grus[empty_ellipses_mask]#.detach().cpu().numpy()\n",
    "    return empty_xs, empty_ellipses, empty_grus, full_ellipses\n",
    "\n",
    "def prolong(x, gru, nearest_hits_mask, nearest_hits, stations_gone, use_torch=False):\n",
    "    if use_torch:\n",
    "        xs_for_prolong = x.unsqueeze(1).repeat((1, nearest_hits_mask.shape[-1], 1,1))\n",
    "        grus_for_prolong = gru.unsqueeze(1)\n",
    "        grus_for_prolong = grus_for_prolong.repeat((1, nearest_hits_mask.shape[-1], 1))\n",
    "        prolonged_xs = torch.zeros((len(xs_for_prolong),\n",
    "                                    nearest_hits_mask.shape[-1],\n",
    "                                    xs_for_prolong.shape[-2] + 1,\n",
    "                                    xs_for_prolong.shape[-1]))\n",
    "        prolonged_xs[:, :, :xs_for_prolong.shape[-2], :] = xs_for_prolong\n",
    "        prolonged_xs[:, :, xs_for_prolong.shape[-2], :] = nearest_hits\n",
    "        #print(prolonged_xs.shape)\n",
    "        prolonged_xs = prolonged_xs.reshape(-1, stations_gone + 1, 3)\n",
    "        nearest_hits_mask = nearest_hits_mask.reshape(-1)\n",
    "        prolonged_xs = prolonged_xs[nearest_hits_mask]\n",
    "        prolonged_grus = grus_for_prolong.reshape(-1, grus_for_prolong.shape[-1])\n",
    "        prolonged_grus = prolonged_grus[nearest_hits_mask]\n",
    "    else:\n",
    "        xs_for_prolong = np.expand_dims(x, 1).repeat(nearest_hits_mask.shape[-1], 1)\n",
    "        grus_for_prolong = np.expand_dims(gru.detach().cpu().numpy(), 1).repeat(nearest_hits_mask.shape[-1], 1)\n",
    "        prolonged_xs = np.zeros(\n",
    "            (len(xs_for_prolong), nearest_hits_mask.shape[-1], xs_for_prolong.shape[2] + 1, 3))\n",
    "\n",
    "        prolonged_xs[:, :, :xs_for_prolong.shape[2], :] = xs_for_prolong\n",
    "        prolonged_xs[:, :, xs_for_prolong.shape[2], :] = nearest_hits\n",
    "        prolonged_xs = prolonged_xs[nearest_hits_mask].reshape(-1, stations_gone + 1, 3)\n",
    "        prolonged_grus = grus_for_prolong[nearest_hits_mask].reshape(-1,\n",
    "                                                                     grus_for_prolong.shape[-2],\n",
    "                                                                     grus_for_prolong.shape[-1])\n",
    "    return prolonged_xs, prolonged_grus\n",
    "\n",
    "def get_candidates(preds, targets, grus, ellipses, index, labels,  labels_for_batch, gru_candidates, track_candidates, candidate_ellipses):\n",
    "    orig_ellipses = torch.zeros((len(ellipses), 4))\n",
    "    orig_ellipses[:, :2] = ellipses[:, :2]\n",
    "    orig_ellipses[:, 2:] = ellipses[:, 3:]\n",
    "    labels_for_ellipses = get_labels_faiss(targets,\n",
    "                                     preds.detach().cpu().numpy(), index=index)\n",
    "    labels_for_batch.append(labels_for_ellipses)\n",
    "    gru_candidates.extend(grus)  # because we need gru for predicted ellipse\n",
    "    labels.extend(labels_for_ellipses)\n",
    "    track_candidates.extend(preds)\n",
    "    candidate_ellipses.extend(orig_ellipses)\n",
    "    return labels, labels_for_batch, gru_candidates, track_candidates, candidate_ellipses\n",
    "\n",
    "def get_labels_faiss(gt_tracks, predicted_tracks, index):\n",
    "    labels_for_ellipses = np.zeros(len(predicted_tracks))\n",
    "    assert len(predicted_tracks) > 0, 'Can not compute labels for empty set of tracks!'\n",
    "    if len(gt_tracks) > 0:\n",
    "        tracks_len = gt_tracks.shape[-2]\n",
    "        gt_tracks = gt_tracks.reshape(-1, gt_tracks.shape[-1])\n",
    "        predicted_tracks = predicted_tracks.reshape(-1, gt_tracks.shape[-1])\n",
    "        gt_index = search_in_index(gt_tracks, index, 1, n_dim=3).flatten().reshape(-1, tracks_len)\n",
    "        predicted_index = search_in_index(predicted_tracks, index, 1, n_dim=3).flatten().reshape(-1, tracks_len)\n",
    "        expanded_gt = np.expand_dims(gt_index, 0).repeat(len(predicted_index), 0)\n",
    "        expanded_preds = np.expand_dims(predicted_index, 1).repeat(expanded_gt.shape[1], 1)\n",
    "        labels_for_ellipses += np.any(np.all(np.equal(expanded_gt, expanded_preds), axis=-1),axis=-1)\n",
    "        assert len(labels_for_ellipses) == len(expanded_preds), 'length of labels and xs is different!'\n",
    "    return labels_for_ellipses\n",
    "\n",
    "def build_df(preds, labels, event_df):\n",
    "    curr_event = event_df.event.values[0]\n",
    "    n_stations = 9\n",
    "    \n",
    "    new_columns = [*[f'hit_id_{i}' for i in range(n_stations)], 'event', 'track_pred']\n",
    "    new_df = pd.DataFrame(columns=new_columns)\n",
    "    new_df.track_pred = new_df.track_pred.astype('bool')\n",
    "    \n",
    "    if len(preds) > 10000:\n",
    "        print(f'{len(preds)} preds found in {curr_event}, skipping')\n",
    "        return new_df\n",
    "\n",
    "    big_index = build_index(event_df)\n",
    "    event_idxs = np.array(event_df.index_old)\n",
    "    tracks_list = []\n",
    "    for idx1, pred in enumerate(preds):\n",
    "        found = searching(pred, big_index)\n",
    "        hit_list = event_idxs[found]\n",
    "        to_pad = n_stations - len(hit_list)\n",
    "        padded_hits = np.pad(hit_list, (0, to_pad), constant_values=-1)\n",
    "        \n",
    "        padded_hits = np.append(padded_hits, curr_event)\n",
    "        padded_hits = np.append(padded_hits, True)\n",
    "        tracks_list.append(padded_hits)\n",
    "    \n",
    "    new_df = pd.DataFrame(tracks_list, columns=new_columns)\n",
    "    new_df.track_pred = new_df.track_pred.astype('bool')\n",
    "    return new_df\n",
    "\n",
    "def searching(points, index):\n",
    "    cont = np.ascontiguousarray(points.cpu())\n",
    "    return search_in_index(cont, index, 1, n_dim=3).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read entry b454a12f953a77a36bdec209025234a1 hit\n",
      "[prepare]: started processing a df simdata_ArPb_3.2AGeV_mb_46.txt with 1283239 rows:\n",
      "read entry 7d8f6db760f5fa6b4c0bbb8354579dc1 hit\n",
      "[prepare] finished\n",
      "[prepare] loading your model(s)...\n",
      "[prepare] finished loading your model(s)...\n",
      "[build_all_tracks] start\n",
      "read entry d5bffb45e73d5372d559f219ef01293a hit\n",
      "read entry c2e5618193c38aa57d62358b64cb868f hit\n",
      "[build_all_tracks] cache hit, finish\n",
      "[run model] start\n",
      "read entry 2b3f80300b54c1942ff70da5e04e3b5d hit\n",
      "read entry f1204caec0519044d6592073a79dd3aa hit\n",
      "\n",
      "\n",
      "processed: 1999: 100%|██████████| 1999/1999 [02:29<00:00, 13.41it/s]\n",
      "[run model] cache miss, finish\n",
      "[solve results] start\n",
      "[solve results] finish\n",
      "[solve results] final stats:\n",
      "==========EVALUATION RESULTS==========\n",
      "Total events evaluated: 1746\n",
      "Total tracks evaluated: 12358\n",
      "Track Efficiency (recall): 0.9114 \n",
      "Track Purity (precision): 0.0208 \n",
      "Fully reconstructed event ratio: 0.6262\n",
      "Mean cpu time per event: 0.0003 sec (3192.70 events per second) \n",
      "Mean gpu time per event: 0.0706 sec (14.17 events per second) \n",
      "==========EVALUATION RESULTS==========\n",
      "solving took 1.4217658042907715 seconds\n"
     ]
    }
   ],
   "source": [
    "N_STATIONS = 9\n",
    "\n",
    "evaluator = EventEvaluator(parse_cfg, global_transformer, N_STATIONS)\n",
    "events = evaluator.prepare(model_loader=TrackNetModelLoader())[0]\n",
    "all_results = evaluator.build_all_tracks()\n",
    "model_results = evaluator.run_model(preprocess_one_event, run_tracknet_eval)\n",
    "start_time = time.time()\n",
    "results_tracknet = evaluator.solve_results(model_results, all_results)\n",
    "end_time = time.time()\n",
    "print(f'solving took {end_time - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building raw track candidates: time mean - 0.037587725203773154, time std - 0.018141825237064712\n"
     ]
    }
   ],
   "source": [
    "times_arr = np.array(times)\n",
    "print(f\"Building raw track candidates: time mean - {times_arr.mean()}, time std - {np.std(times_arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "Data pattern:\n",
    "    - Dict:\n",
    "        - tracks_candidates - List of length N - number of track candidates:\n",
    "            - np.array: shape (M, 3) - M - number of hits per track;\n",
    "        - labels - np.array: shape (N) - labels for track candidates (True - real, False - ghost);\n",
    "        - events - np.array: shape (N) - number of event;\n",
    "        - total_tracks - int: number of real tracks in all events;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATION_COLUMNS = [f\"hit_id_{n}\" for n in range(N_STATIONS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_xyz = events[['x', 'y', 'z']].copy()\n",
    "events_xyz.set_index(events.index_old, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 541894/541894 [01:41<00:00, 5319.23it/s]\n"
     ]
    }
   ],
   "source": [
    "total = len(results_tracknet[0].query('pred != -1'))\n",
    "results_candidates = results_tracknet[0].query('pred != 0')\n",
    "\n",
    "labels = results_candidates.pred.values > 0\n",
    "events_ids = results_candidates.event_id.values\n",
    "\n",
    "track_candidates = []\n",
    "results_candidates = results_candidates[STATION_COLUMNS]\n",
    "for track_candidate in tqdm(results_candidates.itertuples(index=False, name=None), total=len(results_candidates)):\n",
    "    track_candidate = np.array(track_candidate)\n",
    "    track_candidate = track_candidate[track_candidate >= 0]\n",
    "    track_candidates.append(events_xyz.loc[track_candidate].values)\n",
    "\n",
    "result_dict = {'track_candidates': track_candidates,\n",
    "               'labels': labels,\n",
    "               'events': events_ids,\n",
    "               'total_tracks': total\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(result_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"data.pkl\", \"rb\") as f:\n",
    "    output = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9113934293575012"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['labels'][output['labels']]) / output['total_tracks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020784507671241976"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output['labels'][output['labels']]) / len(output['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ariadne_gpu_1_kernel",
   "language": "python",
   "name": "ariadne_gpu_1_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
